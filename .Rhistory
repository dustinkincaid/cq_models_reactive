if(!require(googledrive)) install.packages("googledrive")
library(googledrive)
drive_deauth()
drive_user()
public_file <-  drive_get(as_id("1GIU_sFdLHIid868pCUuK02xdcHuwjZYF"))
drive_download(public_file, overwrite = TRUE)
drive_download(file = public_file,
path = "/data",
overwrite = TRUE)
drive_download(file = public_file,
path = "~data",
overwrite = TRUE)
drive_download(file = public_file,
path = "data/nh_cq_data_filtered.csv",
overwrite = TRUE)
# Fit the CQ data to Hall's models
# Load libraries ----
library("tidyverse")    # general workhorse
library("here")         # takes the guesswork out of dealing with file paths
library("data.table")   # read in large datasets faster
library("reshape2")     # manipulate dataframes
library("patchwork")    # makes laying out plots much easier
library("grid")         # more plotting assistance
library("rsample")      # for model cross-validation
library("doParallel")   # for faster computing
library("scico")        # color palette for ggplot2
library("minpack.lm")   # alternative to 'nls'; much more flexible!
# library("scales")       # used for plotting on log scales
# Which site and solute would you like to model?
site_choice <- "BDC"
sol_choice <- "no3"
grab_choice <- "no3_grab"
# Options ----
options(scipen = 999) #disable printing of numbers in scientific notation
# Functions for model fitting and plotting ----
RMSE <- function(error) { sqrt(mean(error^2)) }
KirchnerBinning = function(df, min_per_bin = 20){
logQ = log(df$q)
logRange = max(logQ) - min(logQ)
minBinSize = logRange*.01
binBoundaries = c(1)
for (i in 2:dim(df)[1]){
if (abs(logQ[i] - logQ[tail(binBoundaries,n=1)]) < minBinSize){
next
}
if (abs(i-tail(binBoundaries,n=1)) < min_per_bin){
next
}
curr = as.numeric(unlist(df[tail(binBoundaries,n=1):i,'c']))
if (sd(curr,na.rm=TRUE)/sqrt(abs(i-tail(binBoundaries,n=1))) > mean(curr)/2){
next
}
binBoundaries=c(binBoundaries,i)
}
return(binBoundaries)
}
# Read in data ----
## CQ data from 1_prep_cq_data.R ----
dat_all <-
data.table::fread(here("data", "nh_cq_data_filtered.csv"))
## C0 choices
# Baseflow
C0_baseflow <-
read_csv(here("data", "c0_baseflow_estimates.csv"))
# Wet deposition
C0_wetdep <-
read_csv(here("data", "wet_dep_mean_concs.csv")) %>%
full_join(tibble(site = c("BDC", "DCF", "LMP", "WHB", "BEF", "HBF"),
source = c("lrho", "lrho", "lrho", "lrho", "hbef", "hbef")))
## Recession 'n' values
rec_n_all <-
read_csv(here("data", "recessionA.csv")) %>%
rename(site = Site)
# Pull C0 and n values ----
c0_bf <-
C0_baseflow %>%
filter(site == site_choice,
var == sol_choice) %>%
pull()
c0_wd <-
C0_wetdep %>%
rename(no3 = no3_mgNL,
doc = doc_mgCL) %>%
filter(site == site_choice) %>%
pull(sol_choice)
rec_n <-
rec_n_all %>%
filter(site == site_choice) %>%
pull(n)
# Create grab sample and sensor datasets ----
# Subset for the site
df_sub <-
dat_all %>%
filter(site == site_choice)
# Filter for 15-min sensor data
df_15min <-
df_sub %>%
filter(!is.na(!!as.symbol(sol_choice))) %>%
filter(!is.na(q)) %>%
rename(c = !!as.symbol(sol_choice))
# Filter for long-term grab sample data
df_grab <-
df_sub %>%
filter(!is.na(!!as.symbol(grab_choice))) %>%
filter(!is.na(q)) %>%
rename(c = !!as.symbol(grab_choice))
# Reduce sensor data to the various sampling times (ie, daily, weekly, monthly) ----
# Here we create 10 versions of each sampling frequency (e.g., df_15min_daily_1 through df_15min_daily_10)
# Daily (sample always occurs sometime between 8am and 5pm)
df_15min_daily_list <- list()
for(i in 1:10){
df_15min_daily_list[[i]] <-
df_15min %>%
mutate(period = ifelse(lubridate::hour(datetime) %in% c(8:16), "sampling", "not_sampling")) %>%
filter(period == "sampling") %>%
group_by(date) %>%
# This was randomly sample a row from each group (here, date)
slice_sample(n = 1) %>%
mutate(df = "daily") %>%
ungroup()
}
# Weekly (sample always occurs M-F sometime between 8am and 5pm)
df_15min_weekly_list <- list()
for(i in 1:10){
df_15min_weekly_list[[i]] <-
df_15min %>%
mutate(period = ifelse(lubridate::hour(datetime) %in% c(8:16), "sampling", "not_sampling")) %>%
filter(period == "sampling") %>%
mutate(wday = lubridate::wday(date, label = TRUE)) %>%
filter(!wday %in% c("Sat", "Sun")) %>%
group_by(lubridate::year(date), lubridate::week(date)) %>%
# This was randomly sample a row from each group (here, date)
slice_sample(n = 1) %>%
mutate(df = "weekly") %>%
ungroup() %>%
select(-c(`lubridate::year(date)`, `lubridate::week(date)`))
}
# Monthly (sample always occurs M-F sometime between 8am and 5pm)
df_15min_monthly_list <- list()
for(i in 1:10){
df_15min_monthly_list[[i]] <-
df_15min %>%
mutate(period = ifelse(lubridate::hour(datetime) %in% c(8:16), "sampling", "not_sampling")) %>%
filter(period == "sampling") %>%
mutate(wday = lubridate::wday(date, label = TRUE)) %>%
filter(!wday %in% c("Sat", "Sun")) %>%
group_by(lubridate::year(date), lubridate::month(date)) %>%
# This was randomly sample a row from each group (here, date)
slice_sample(n = 1) %>%
mutate(df = "monthly") %>%
ungroup() %>%
select(-c(`lubridate::year(date)`, `lubridate::month(date)`))
}
# Fit the models with each dataset  ----
## Create list of datasets ----
df_list <- list(df_grab %>% mutate(df = "lt_grab"),
df_15min %>% mutate(df = "15min"),
df_15min_daily_list[[1]],
df_15min_weekly_list[[1]],
df_15min_monthly_list[[1]])
## Fit models, save residuals, and create plot df ----
### c0 = c0_bf ----
c0_set <- c0_bf
# i = 1
resids_final <- tibble()
bdf_all <- tibble()
plot_df_all <- tibble()
